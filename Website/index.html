<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>C-SAG Computer Vision Project</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>

	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<div class="author-img" style="background-image: url(images/fetch.jpg);"></div>
				<h1 id="colorlib-logo"><a href="index.html">C-SAG</a></h1>
				<span class="position"><a href=""> Computer Vision Group</a> for CS6476</span>
			</div>
			<nav id="colorlib-main-menu" role="navigation" class="navbar">
				<div id="navbar" class="collapse">
					<ul>
						<li class="active"><a href="#" data-nav-section="home">Home</a></li>
						<li><a href="#" data-nav-section="problem">Problem Statment</a></li>
						<li><a href="#" data-nav-section="abstract">Abstract</a></li>
						<li><a href="#" data-nav-section="teaser">Teaser</a></li>
						<li><a href="#" data-nav-section="intro">Introduction</a></li>
						<li><a href="#" data-nav-section="approach">Approach</a></li>
						<li><a href="#" data-nav-section="experiment">Experiments</a></li>
						<li><a href="#" data-nav-section="results">Results</a></li>
						<li><a href="#" data-nav-section="qual">Qualitative Results</a></li>
						<li><a href="#" data-nav-section="video">Videos</a></li>
						<li><a href="#" data-nav-section="conclusion">Conclusion</a></li>
						<li><a href="#" data-nav-section="future">Future Work</a></li>
						<li><a href="#" data-nav-section="ref">References</a></li>
						<li><a href="#" data-nav-section="team">Team</a></li>
					</ul>
				</div>
			</nav>

			<div class="colorlib-footer"
					<ul>
						<li><a href="gneville6@gatech.edu"><i class="icon-mail"></i></a>Email  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   </li>
						<li><a href="https://github.com/gneville6/C-SAG"><i class="icon-github"></i></a>GitHub  &nbsp;&nbsp;&nbsp;</li>
						<li><a href="https://samyak-268.github.io/F19CS6476/"><i class="icon-libreoffice"></i></a> Project Description </li>
						<li><a href="https://drive.google.com/open?id=1EDkkk49trY8xo3rJ_FpPpjz-nWgR8PLe"><i class="icon-libreoffice"></i></a> Dataset </li>
					</ul>
			</div>

		</aside>

		<div id="colorlib-main">
			<section id="colorlib-hero" class="js-fullheight" data-section="home">
				<div class="flexslider js-fullheight">
					<ul class="slides">0
				   	<li style="background-image: url(images/laptop.jpg); background-position: left; ">
				   		<div class="overlay"></div>
				   		<div class="container-fluid">
				   			<div class="row">
					   			<div class="col-md-6 col-md-offset-3 col-md-pull-3 col-sm-12 col-xs-12 js-fullheight slider-text">
					   				<div class="slider-text-inner js-fullheight">
					   					<div class="desc">
						   					<h1>Hi! <br>We're C-SAG</h1>
						   					<h2>a computer vision group in the Georgia Tech CS6476 </h2>
											<h2>Class and below is our final project</h2>
												<p><a href="https://github.com/gneville6/C-SAG" class="btn btn-primary btn-learn">See the Code <i class="icon-download4"></i></a></p>
											</div>
					   				</div>
					   			</div>
					   		</div>
				   		</div>
				   	</li>
				  	</ul>
			  	</div>
			</section>

			<section class="colorlib-about" data-section="problem">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">About the Problem?</span>
										<h2 class="colorlib-heading">Problem Statement</h2>
										<p>
											A team of two robots (A and B) is attempting to perform a set of tasks in parallel. Each task involves multiple actions such as move to this position, pick up this item, etc. If robot A fails a specific task/action or is significantly slower at performing the task/action than expected (possibly due to errors), how can this failure or error be detected and resolved so that the time taken to complete all of the tasks is still optimized? In the base case, a failure will go unnoticed and the set of tasks for robot A will be compromised. In the case of a robot being slower than expected this could result in missing deadlines or hold up future work that depends on the results of this work. If the error or slowdown is caught in a sufficient amount of time, the error or slowdown can be corrected or mitigated and the set of tasks performed by robot A will still yield positive results. Our goal for this project is to build out the vision systems needed to recognize a robot is erroring in a simple pick and place task.
										</p>
										<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>


			</section>

			<section class="colorlib-about" data-section="abstract">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Goals</span>
										<h2 class="colorlib-heading">Abstract</h2>
										<p>
										Multi-agent systems in the real world require collaboration between robotic agents in order to effectively accomplish tasks. However, robotic errors can hamper the performance of a multi-agent system. Our hope is to provide a method that allows robots to cross-check the progress of other agents and plan around these failures to ensure the team's successful completion of a set of tasks. For this paper, we focus specifically on the computer vision part of a task where another robot has attempted to pick and place gears and bolts into a bin. The supervising robot must detect what was placed into the bin and determine if the other robot has made an error such as putting the wrong part in, putting too many or too few of a specific part in, or stopped moving entirely. For this task, many approaches were taken including a convolutional neural network and several more classical vision techniques. Also, the supervising robot must be capable of knowing if the robot has errored out and is no longer working to complete the task. In order to recognize this task we investigated two forms of motion detection to determine if the robot has moved over some time period.</p>
										<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>


			</section>

			<section class="colorlib-about" data-section="teaser">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Just a Sample (Scroll over for more information)</span>
										<h2 class="colorlib-heading">Teaser</h2>
										<div class="row">
												<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
													<div class="project" style="background-image: url(images/teaser1.png);">
														<div class="desc">
															<div class="con">
																<h3><a href="#" data-nav-section="home">Robot Task</a></h3>
																<span>Robot performing pick and place example task.
																	Our algorithms are deigned to detect errors that a robot might face performing this task.
																	Our error checking algorithms are designed to check the number of parts in the bucket and whether or not the robot is still working on the goal (is moving).
																	If the robot stops moving or does not have enough parts in the bin the system will detect an error.
																	In the future we hope to implement this on a team of robots and adjust the robots plans according to sensed errors.
																</span>
															</div>
														</div>
													</div>
												</div>
												<div class="col-md-6 animate-box" data-animate-effect="fadeInRight">
													<div class="project" style="background-image: url(images/teaser2.png);">
														<div class="desc">
															<div class="con">
																<h3><a href="#" data-nav-section="home">Bin </a></h3>
																<span>A picture of the bin that the robot is pick parts from</span>
															</div>
														</div>
													</div>
												</div>
											</div>
										<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>


			</section>

			<section class="colorlib-about" data-section="intro">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Intro to Our Work</span>
										<h2 class="colorlib-heading">Introduction</h2>
										<p>
										The goal of this project is to enable robots to monitor each other periodically using computer vision to mitigate the errors that may occur. For example, robot A, while working on completing its own set of tasks, can glance over and observe robot B operating in its workspace and determine if everything is going smoothly or if an intervention of some type is needed.
										</p>
										<p>
										Our approach is different from most existing approaches to monitoring faults in multi-robot systems [4, 5] as most current approaches do not use vision. By utilizing vision, our system is not dependent on outside factors such as the network connection or internal sensors to influence whether the robots will be able to communicate with each other. Therefore teammates are able to take over tasks and complete overall objectives.
										</p>
										<p>
										For this project, we are focusing on the computer vision part of the supervising robot’s error detection. This involves determining what the other robot has already placed in the bin and knowing whether the other robot has placed too little or too much of each object. It also uses a motion image to determine if the other robot has stopped moving.

										</p>
										<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>


			</section>

			<section class="colorlib-about" data-section="approach">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">How we did it</span>
										<h2 class="colorlib-heading">Approach</h2>
										<p>
										Several different approaches were used by the supervisory robot to determine what objects were placed in a bin by another robot. These include histogram thresholding, contour counting, probability distributions, and a convolutional neural network. Several methods were also used for motion detection (MHI and gaussian mixture separation model).
											Finally a dataset of labeled images was created to train our learning algorithms on. This dataset had images of a variety of orientations and scales of various configurations of the bin and parts.
										</p>
										<div class="col-md-6 animate-box" data-animate-effect="fadeInRight">
													<div class="project" style="background-image: url(images/teaser2.png);">
														<div class="desc">
															<div class="con">
																<h3><a href="#" data-nav-section="home">Example image from dataset </a></h3>
																<span>A picture of the bin that the robot is pick parts from</span>
															</div>
														</div>
													</div>
										</div>
										<h4>Data Collection</h4>
										<p>To aid in the error detection process, data was manually collected in 25 different specific classes. The classes were based on the count of each object in the bin. For example the figure above shows 1 large gear in red, 1 small gear in blue and 2 bolts in grey, therefore the label for this image is “2bolts1big1small.” To collect the data, we recorded video from the Fetch robot’s camera while shaking the green bin to vary the position of the gears and bolts. There was approximately 1 hour of video recorded in total. After converting the video into individual frames, there were approximately 10,000 images in the total dataset with approximately 400 images per class. The advantage of this method is that we can label each image from a specific video with a certain class which minimizes the amount of image labeling required. One note on the classes, the ultimate goal is to determine if the bin has the parts necessary to build a full kit. By initially splitting the data into 25 classes, the option to consolidate into small groups of classes remained such as combining all the images with the large gear to help create a classifier to check for the large gear. </p>


										<h4>Histogram Thresholding</h4>
										<p>The first and simplest of the approaches we tested for classifying the items in the bin was a thresholding based approach in this approach we would threshold based on the green of the box and locate the contour of the outside of the box using opencv find contour function. After finding the contour we would remove all of the data outside of the region of the box by setting the pixel values to black. After doing this we would threshold away the green of the box and perform a slight erosion and dilation  on the mask to remove any small blobs and to prevent any of the gears/bolts from being removed from the image. Once this was done we would have an image that only contained the gears/bolts. Finally we would threshold themimage containing only the gear/bolts on each of the colors: red, blue and gray and count the numbers of pixels of each color in the image. Using these histograms of colors we would detect the number of gears/bolts by setting ranges on the number of pixels. The exact threshold values were hand tuned to maximize the performance of the classifier on the dataset. This method was tested by measuring its performance on the dataset collected above and qualitatively by running the classifier on webcam images while moving and changing the number of objects in the bin.</p>


										<h4>Contour Counting</h4>
										<p>In this method, images were cropped to the boundary of the box, thresholded on green to cut out the back of the box, dilated and eroded to reduce noise, and finally run through a contour detection on the masked image. Finally, the total number of contours detected larger than a set area was used to predict how many screws/bolts were in the box. </p>


										<h4>Probability Distributions</h4>
										<p>In this method, images were cropped to the boundary of the box. A percentage of the box that contained each of the three types of objects was computed. Classical computer vision methods, including color thresholding, image segmentation, and edge detection, were used to determine which pixels belonged to each type of object. These three percentages were then computed for each of the labeled images in the dataset to create probability distributions for each of the classes. This classifier uses these probability distributions to predict a class for test images based on the percentage of the box that is each of the three types of objects. </p>


										<h4>Deep Learning</h4>
										<p>Convolutional Neural Networks were utilized to classify the images into the 25 different classes which ultimately provides the correct count of each object in the bin.  All images were cropped and padded to ensure that the aspect ratio was maintained and the images were the correct size as they were fed into the networks. A few different networks were trained. The first network was trained on the raw images collected. The second network was trained on images cropped to out line of the green bin. The third network trained on images cropped and thresholded on the green background. The fourth used images that were cropped, thresholded and then small blobs that the threshold missed
										were removed.
										</p>
										<h5>Tools</h5>
										<p>To build, train and test the CNN models, the Keras library with Tensorflow backend was used [Chollet] All training was done on a Windows 10 laptop with 16 GB of RAM and an NVIDIA GeForce 940MX.
										</p>
										<h5>Model Design and Parameters</h5>
										<p>From a high level, the model was custom built with two convolutional layers with max pooling and one dense fully connected layer and a final soft max layer. The convolutional layers contained 32 3x3 filters. This simple architecture was consistent for all the trained networks. Existing model architectures were tested however they would crash due to the limitation of the hardware used. The model pipeline is available below in figure. XXX.The input image size was (128,128,3) and a batch size of 32 was used. This resulted in a network with approximately 3,000,000 trainable parameters. Most of these decisions were made to limit the overall size of the network to work effectively on the available hardware.
										A model of the tested network is shown below.
											<img src="images/model.png" alt="acc graph" width=100% align="center"></p>
										<h5>Training</h5>
										<p>The images were broken into two sets the training and validation sets. The validation set was generated by randomly selecting 10% of the data from each class. This was done for each of the levels of preprocessed images. Each of the four neural networks were trained for approximately 30 epochs. The training was stopped short if the training set accuracy reached 100% accuracy which happened for all data sets around 30 epochs. Figure XXXX shows a sample of the training over the course of 15 epochs.</p>
										<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
											<div class="project" style="background-image: url(images/cnn1.png);">
												<div class="desc">
													<div class="con">
														<h3><a href="#" data-nav-section="home">CNN Accuracy</a></h3>
														<span>A sample of training run over 15 epochs on the maximally processed image data set where images were cropped, thresholded and blob removal. Shows the accuracy of the training and test.
														</span>
													</div>
												</div>
											</div>
										</div>
										<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
											<div class="project" style="background-image: url(images/cnn2.png);">
												<div class="desc">
													<div class="con">
														<h3><a href="#" data-nav-section="home">CNN Loss</a></h3>
														<span>A sample of training run over 15 epochs on the maximally processed image data set where images were cropped, thresholded and blob removal. Top shows the calculated loss of the training and test.
														</span>
													</div>
												</div>
											</div>
										</div>
										<h4>Motion Detection</h4>
										<p>In order to detect if the robot has errored and is no longer moving a motion detection system needed to be implemented. In order to do this we tried two different approaches. The first approach was to use a motion history image to measure the change in the frame over time. The second method was to use a motion detection algorithm MOG2 which uses gaussian mixtures to remove the background. Both of these methods were tested empirically using streams of video from a webcam.
										</p>


									</div>
								</div>
							</div>

						</div>
					</div>
				</div>


			</section>

			<div id="colorlib-counter" class="colorlib-counters" style="background-image: url(images/tech-tower.jpg);" data-stellar-background-ratio="0.5">
				<div class="overlay"></div>
				<div class="colorlib-narrow-content">
					<div class="row">
					</div>
					<div class="row">
						<div class="col-md-3 text-center animate-box">
							<span class="colorlib-counter js-counter" data-from="0" data-to="4000" data-speed="5000" data-refresh-interval="50"></span>
							<span class="colorlib-counter-label">+ Lines of Code</span>
						</div>
						<div class="col-md-3 text-center animate-box">
							<span class="colorlib-counter js-counter" data-from="0" data-to="700" data-speed="5000" data-refresh-interval="50"></span>
							<span class="colorlib-counter-label">Cups of coffee</span>
						</div>
						<div class="col-md-3 text-center animate-box">
							<span class="colorlib-counter js-counter" data-from="0" data-to="7" data-speed="5000" data-refresh-interval="50"></span>
							<span class="colorlib-counter-label">Weeks worked on</span>
						</div>
						<div class="col-md-3 text-center animate-box">
							<span class="colorlib-counter js-counter" data-from="0" data-to="4" data-speed="5000" data-refresh-interval="50"></span>
							<span class="colorlib-counter-label">Teammates</span>
						</div>
					</div>
				</div>
			</div>

			<section class="colorlib-about" data-section="experiment">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Experiments Run</span>
										<h2 class="colorlib-heading">Experiment and Results</h2>
										<p>
										For each of the classifiers we tested them by measuring there performance on a test set extracted from the dataset. Below you can see the accuracy values and the confusion matrices for all tested classifiers
										</p>
<!--
-->
									<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>


			</section>

			<section class="colorlib-education" data-section="results">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
							<span class="heading-meta">What did we find</span>
							<h2 class="colorlib-heading animate-box">Results</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-12 animate-box" data-animate-effect="fadeInLeft">
							<div class="fancy-collapse-panel">
								<div class="panel-group" id="accordion" role="tablist" aria-multiselectable="true">
									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingOne">
									        <h4 class="panel-title">
									            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" aria-expanded="true" aria-controls="collapseOne">Data Collection</a>

									        </h4>
									    </div>
									    <div id="collapseOne" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="headingOne">
									         <div class="panel-body">
									            <div class="row">
										      		<div class="col-md-6">
										      		<h4>Data Collection</h4>
										      			<p>
															In order to collect our dataset to train our classifiers we took a video of the container with a group of parts. During the video, we shake the box to allow for the bolts and screws to fall into a variety of positions and orientations. After this, we separate each of the frames of the video into individual images. Since all of the images have the same number and types of parts we can label the entire sequence of images collected in this manner using the same label. After this, we change the number and type of parts in the bin and take another video. We continue doing this until all combinations of parts have been collected. A link too our dataset is in the side panel.
														</p>
										      		</div>

										      	</div>
									         </div>
									    </div>
									</div>
									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingTwo">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">Histogram Thresholding
									            </a>
									        </h4>
									    </div>
									    <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
									        <div class="panel-body">
									        <h4>Histogram Thresholding</h4>
									            <p>
														This method was tested by measuring its performance on the dataset discussed above and qualitatively by running the classifier on webcam images while moving and changing the number of objects in the bin. Below is the confusion matrices of each of the object classes. This method performed very well on both the small and large gears. This is due to the fact that the colors of these objects were easy to threshold out. The bolts however performed poorly. This is due to the fact that gray was difficult to threshold on due to glare. Due to poor thresholding of the color the number of pixels varied wildly and the classifier performed poorly. The accuracies for the large gear, small gear and bolts with this method were calculated and are shown below compared to the baseline of random classification.
												</p>
												<img src="images/hist1.png" alt="acc graph" width="500"  align="middle">
												<img src="images/hist2.png" alt="Big gear confusion matrix" width="500"  align="middle">
												<img src="images/hist3.png" alt="Small gear confusion matrix" width="500"  align="middle">
												<img src="images/hist3.png" alt="Bolt confusion matrix" width="500"  align="middle">
									        </div>
									    </div>
									</div>
									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingThree">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="false" aria-controls="collapseThree">Contour Counting
									            </a>
									        </h4>
									    </div>
									    <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
									            <div class="panel-body">
									            <h4>Contour Counting</h4>
									            <p>
													The tables below shows the confusion matrix for the three different categories. Similar to the other methodologies, we have relatively high accuracies for the big red gear. However, detecting grey bolts still cause a bit of an issue. This is because thresholding the image for grey caused the glare to be included in the image as well.
												</p>
												<img src="images/cont1.png" alt="acc graph" width="500"  align="middle">
												<img src="images/cont2.png" alt="Big gear confusion matrix" width="500"  align="middle">
												<img src="images/cont3.png" alt="Small gear confusion matrix" width="500"  align="middle">
												<img src="images/cont4.png" alt="Bolt confusion matrix" width="500"  align="middle">
									        </div>
									    </div>
									</div>

									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingFour">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseFour" aria-expanded="false" aria-controls="collapseFour"> Probability Distributions
									            </a>
									        </h4>
									    </div>
									    <div id="collapseFour" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingFour">
									        <div class="panel-body">
									        <h4>Probability Distributions </h4>
									            <p>
													As shown in the tables below, using probability distributions based on features of the image was pretty successful for counting the number of red and blue gears and had the most success out of the methods tried for determining the number of gray bolts. Below the tables are the probability distributions for each of the objects.
												</p>
												<img src="images/prob1.png" alt="acc graph" width="500"  align="middle">
												<img src="images/prob2.png" alt="Big gear confusion matrix" width="500"  align="middle">
												<img src="images/prob3.png" alt="Small gear confusion matrix" width="500"  align="middle">
												<img src="images/prob4.png" alt="Bolt confusion matrix" width="500"  align="middle">
												<img src="images/prob5.png" alt="Big gear confusion matrix" width="500"  align="middle">
												<img src="images/prob6.png" alt="Small gear confusion matrix" width="500"  align="middle">
												<img src="images/prob7.png" alt="Bolt confusion matrix" width="500"  align="middle">
									        </div>
									    </div>
									</div>

									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingFour">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseFive" aria-expanded="false" aria-controls="collapseFive">Deep Learning
									            </a>
									        </h4>
									    </div>
									    <div id="collapseFive" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingFive">
									        <div class="panel-body">
									        	<h4>Deep Learning</h4>
									            <p>
													The table below shows a summary of the validation set accuracies at the end of training. While it is surprising to see that preprocessed images performing so poorly, we believe this is due to the introduction of the random edges resulting from the thresholding process.
												</p>
												<p>Top is the confusion matrix when run against the validation set for the cropped images. It can be seen clearly that it misses a couple classes severely and tends to favor class 21. Bottom is the confusion matrix for the raw images. It shows a nice diagonal correctly classifying most of the matrix.
</p>
												<img src="images/cnn3.png" alt="Bolt confusion matrix" width="500"  align="middle">
												<img src="images/cnn4.png" alt="Bolt confusion matrix" width="500"  align="middle">
												<img src="images/cnn5.png" alt="Bolt confusion matrix" width="500"  align="middle">

												<h4>Live Inference test</h4>
									            <p>We ran live inference using the model built on the cropped image set using an external web-cam. The inference failed miserably. The classifications did not seem to match the classes at all based on the brief video inference. We were running the inference on each frame independently as it was received into the camera feed. The figures below shows two instances where of these failures.</p>
									        	<img src="images/cnn6.png" alt="Bolt confusion matrix" width="500"  align="middle">
												<img src="images/cnn7.png" alt="Bolt confusion matrix" width="500"  align="middle">


											</div>
									    </div>
									</div>

									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingFour">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseSix" aria-expanded="false" aria-controls="collapseSix">Motion Detection
									            </a>
									        </h4>
									    </div>
									    <div id="collapseSix" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingSix">
									        <div class="panel-body">
									        <h4>Motion Detection</h4>
									            <p>
													In order to test the results we ran several empirical tests using webcam video stream. The first method tested was the MHI approach. This method performed poorly due to a high amount of noise in the image pixels between frames. Due to the noise this method proved inviable. The second method we looked at was an MOG2 motion detection algorithm that used gaussian mixtures. This method performed very well and was able to very accurately locate all motion in the image. Using this model we were able to create an error checking script where the camera watches the robot and detects motion. If motion has not been detected for a set amount of time  the algorithm will say that the robot is in an error state. Below are several pictures that give an understanding of how these algorithms performed.
												</p>
												<img src="images/motion1.png" alt="acc graph" width="500"  align="middle">
												<img src="images/motion2.png" alt="Big gear confusion matrix" width="500"  align="middle">
												<img src="images/motion3.png" alt="Small gear confusion matrix" width="500"  align="middle">
												<img src="images/motion4.png" alt="Bolt confusion matrix" width="500"  align="middle">
									        </div>
									    </div>
									</div>

								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-work" data-section="qual">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
							<span class="heading-meta">Pictures of Work</span>
							<h2 class="colorlib-heading animate-box">Qualitative Results</h2>
						</div>
					</div>
					<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
						<!--
						<div class="col-md-12">
							<p class="work-menu"><span><a href="#" class="active">Graphic Design</a></span> <span><a href="#">Web Design</a></span> <span><a href="#">Software</a></span> <span><a href="#">Apps</a></span></p>
						</div>
						-->
					</div>
					<h4>Histogram Thresholding</h4>
					<div class="row">
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/qhist1.png);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Histogram Method Performance</a></h3>
										<span>After designing and validating the thresholding method as shown above, we ran the classifier on a webcam while changing the items in the bin to see how well it performed on live images. The results performed similarly to the test set and below are a few pictures of this process.
										</span>
									</div>
								</div>
							</div>
						</div>
						<div class="col-md-6 animate-box" data-animate-effect="fadeInRight">
							<div class="project" style="background-image: url(images/qhist2.png);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Histogram Method Performance</a></h3>
										<span>Here is an example of a test set example we labeled incorrectly</span>
									</div>
								</div>
							</div>
						</div>
					</div>
					<h4>Contour Counting</h4>
					<div class="row">
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/qCont1.jpg);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Contour Method Performance</a></h3>
										<span>The original image</span>
									</div>
								</div>
							</div>
						</div>
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/qCont2.png);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Contour Method Performanc</a></h3>
										<span>The mask for red which produces a very clear detection of the red gear.</span>
									</div>
								</div>
							</div>
						</div>
						<div class="col-md-6 animate-box" data-animate-effect="fadeInRight">
							<div class="project" style="background-image: url(images/qCont3.png);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Contour Method Performanc</a></h3>
										<span>Thresholding of grey pixels which shows clear interference from the glare on the bottom of the box.
										</span>
									</div>
								</div>
							</div>
						</div>
					</div>
					<h4>Probability Distributions</h4>
					<div class="row">
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/qProb1.jpg);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Probability Distribution Method Performance</a></h3>
										<span>This is an image that shows some of the object detection from this classifier. The top left is the red gear, the bottom middle is the gray bolts, and the bottom right is the blue gear. Shown in the bottom middle is an artifact of the glare from the overhead lights that contributed to the reduction in accuracy.</span>
									</div>
								</div>
							</div>
						</div>
					</div>
					<h4>Deep Learning</h4>
					<div class="row">
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/two_fetch.jpg);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">CNN</a></h3>
										<span>Comment 1</span>
									</div>
								</div>
							</div>
						</div>
						<div class="col-md-6 animate-box" data-animate-effect="fadeInRight">
							<div class="project" style="background-image: url(images/comp_vis.jpg);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">CNN</a></h3>
										<span>Comment 2</span>
									</div>
								</div>
							</div>
						</div>
					</div>
					<h4>Motion Detection</h4>
					<div class="row">
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/motion5.png);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Motion Detection</a></h3>
										<span>Example of the motion detectors output when the robot is moving in frame</span>
									</div>
								</div>
							</div>
						</div>
						<div class="col-md-6 animate-box" data-animate-effect="fadeInRight">
							<div class="project" style="background-image: url(images/motion6.png);">
								<div class="desc">
									<div class="con">
										<h3><a href="#" data-nav-section="home">Motion Detection </a></h3>
										<span>Example of the motion detectors output when the robot is in frame but has not moved in a long time</span>
									</div>
								</div>
							</div>
						</div>
					</div>


				</div>
			</section>

			<section class="colorlib-about" data-section="video">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Videos we shot</span>
										<h2 class="colorlib-heading">Videos</h2>
										<p>
											Below is a collection of videos we shot of our project
										</p>
										<h4>Example of Robot Performing a pick and place task</h4>
										<video width=100% height="400" controls>
										  <source src="images/robotScrew.mp4" type="video/mp4">
										  <source src="movie.ogg" type="video/ogg">
										Your browser does not support the video tag.
										</video>
										<h4>CNN Classifier Video</h4>
										<video width=100% height="400" controls>
										  <source src="images/cnn_classifier.mp4" type="video/mp4">
										  <source src="movie.ogg" type="video/ogg">
										Your browser does not support the video tag.
										</video>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-about" data-section="conclusion">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">What did we learn?</span>
										<h2 class="colorlib-heading">Conclusion</h2>
										<p>
											Due to the fact that results from the experiment are still pending, concluding statements can not be made about the validity of the work. However, if this method is effective, there are several interesting avenues available for future work. If we are able to consistently visually detect errors, we may be able to use this information to modify the robot’s list of possible actions to remove actions that have a higher chance of causing errors. Once modified this new action space could be useful in helping develop a dynamic planning algorithm. Also, if agents are capable of recognizing the errors of other robots in a cluster this information could be used to develop algorithms for decentralized cluster behaviors. Finally work like this could be used to develop error recovery strategies that can allow for autonomous correction of errors that have already occurred.

										</p>
										<h4>Conclusion Deep Nets</h4>
										<p>The CNN classifier network seemed to fail pretty dramatically due to over-fitting. It was able to accurately classify the validation and Test sets very nicely, but failed the live inference. It was likely classifying based on extraneous features that are not obvious to us that happened to be consistent across the images whether it be something in the background or the angle of the green bin.
										</p>
										<p>There are a number of things we would like to have improved about the deep learning approach and would continue on this path if more time were available. Firstly, there are adjustments that can be made to the model such as adding additional convolutional layers or using transfer learning by capturing the pre-trained feature filters from existing datasets and then training the fully connected layers. We were limited by the available hardware, but moving forward we would seek to utilize better hardware that can handle larger memory capacity. These improvements are contingent on increasing the size of the dataset. While we did have approximately 10,000 images, they were not unique images and did not provide enough variety for the model to learn the distinguishing and important features. Lastly, we believe that an entirely different approach using an object detection as opposed to classification would yield better results. Overall, we learned that utilizing deep learning requires a good deal of parameter tuning as well as trial and error to be successful.
										</p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-about" data-section="future">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Whats Next?</span>
										<h2 class="colorlib-heading">Future Work</h2>
										<p>
											The results from several of the methods we tried for classify the parts performed poorly (multi-class CNN, binary CNN, and contour counting). We believe that these methods did not perform as well as expected to the size and quality of our data set. The images were too similar and far too few to effectively learn how to count the parts in the bin. Two of the methods that we tried (probabilistic methods and thresholding) did seem to give pretty good results on counting the parts in the bin.
										</p>
										<p>
											Several further improvements could be made to increase the performance of our classifiers. Improving the dataset to remove blurry or images with occlusion and expand the number of images for each class. These improvements to class we believe would improve the performance of the CNN as well as allow us to further tune the classical methods described above.
										</p>
										<p>
											Also in future work we hope to implement these systems on teams of robots. Using these methods we could find which actions cause errors most often and modify the action space. Once modified this new action space could be useful in helping develop a dynamic planning algorithm. Also, if agents are capable of recognizing the errors of other robots in a cluster this information could be used to develop algorithms for decentralized cluster behaviors. Finally work like this could be used to develop error recovery strategies that can allow for autonomous correction of errors that have already occurred.
										</p>
										<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-about" data-section="ref">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">References and Citations</span>
										<h2 class="colorlib-heading">References</h2>
										<p>
											[1] Chollet, François. “Keras.” Keras, GitHub, 2015, GitHub.
										</p>
										<p>
										[2] Banerjee, Siddhartha, et al. "Taking Recoveries to Task: Recovery-Driven Development for Recipe-based Robot Tasks."
										</p>
										<p>
										[3] Ortenzi, Valerio et al. “Vision-Based Framework to Estimate Robot Configuration and Kinematic Constraints.” IEEE/ASME Transactions on Mechatronics 23 (2018): 2402-2412.
										</p>
										<p>
										[4] X. Li and L. E. Parker, "Sensor Analysis for Fault Detection in Tightly-Coupled Multi-Robot Team Tasks," Proceedings 2007 IEEE International Conference on Robotics and Automation, Roma, 2007, pp. 3269-3276.
										</p>
										<p>
										[5] E. Khalastchi and M. Kalech, “Fault Detection and Diagnosis in Multi-Robot Systems: A Survey,” Sensors, vol. 19, no. 18, 2019.
										</p>
										<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-about" data-section="Team">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Who are We?</span>
										<h2 class="colorlib-heading">Team</h2>
										<p>
											Andrew Messing,
										</p>
										<p>
											Glen Neville,
										</p>
										<p>
											Carter Price,
										</p>
										<p>
											Sean Ye
										</p>
										<p></p>
									</div>
								</div>
							</div>

						</div>
					</div>
				</div>
			</section>

		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>





<!-- 
TO DO 
1)fix form
2)form drop
3)project
4)master paper




-->

